{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BertGRU-based AAN  models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### All the test is based on torch-1.2.0 and torchtext-0.6.0\n",
    "\n",
    "\n",
    "import torch\n",
    "from torchtext import data\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from torch.nn import functional as F\n",
    "from model.tools import categorical_accuracy,epoch_time\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare data loader based on torchtext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "init_token = tokenizer.cls_token\n",
    "eos_token = tokenizer.sep_token\n",
    "pad_token = tokenizer.pad_token\n",
    "unk_token = tokenizer.unk_token\n",
    "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
    "eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
    "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
    "\n",
    "def tokenize_and_cut(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence) \n",
    "    tokens = tokens[:max_input_length-2]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def get_iterator_feature(source_file, target_file, BATCH_SIZE=128):\n",
    "    TEXT = data.Field(batch_first = True,\n",
    "                  use_vocab = False,\n",
    "                  tokenize = tokenize_and_cut,\n",
    "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "                  init_token = init_token_idx,\n",
    "                  eos_token = eos_token_idx,\n",
    "                  pad_token = pad_token_idx,\n",
    "                  unk_token = unk_token_idx)\n",
    "\n",
    "    LABEL = data.LabelField(dtype = torch.long)\n",
    "\n",
    "    fields = {'review': ('text', TEXT), 'label': ('label', LABEL)}\n",
    "    # source_file = 'elec.json'\n",
    "    train_data = data.TabularDataset.splits(\n",
    "                        path = 'datasets'+os.sep+\"amazon_text\",\n",
    "                        train = source_file,\n",
    "                        format = 'json',\n",
    "                        fields = fields\n",
    "    )\n",
    "\n",
    "    test_data = data.TabularDataset.splits(\n",
    "                            path = 'datasets'+os.sep+\"amazon_text\",\n",
    "                            train = target_file,\n",
    "                            format = 'json',\n",
    "                            fields = fields\n",
    "    )\n",
    "\n",
    "    train_data = train_data[0]\n",
    "\n",
    "    test_data = test_data[0]\n",
    "\n",
    "\n",
    "    test_data, valid_data = test_data.split(random_state = random.seed(SEED), split_ratio=0.98)\n",
    "\n",
    "\n",
    "    LABEL.build_vocab(train_data)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    source_iterator, target_iterator,valid_iterator = data.BucketIterator.splits(\n",
    "        (train_data, test_data, valid_data), \n",
    "        batch_size = BATCH_SIZE, \n",
    "        sort=False,\n",
    "        shuffle = True,\n",
    "        device = device)\n",
    "\n",
    "    return source_iterator, target_iterator, valid_iterator, TEXT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize AAN model.\n",
    "\n",
    "Two version of AAN you can choose: \"AAN\" and its adversarial version \"AAN-A\". set *aan_version*='AAN' or 'AAN-A' to select different version.\n",
    "\n",
    "For AAN, you is required to set a hyperparameter *MU*, default: *0.1*. \n",
    "For AAN-A, *MU* is an invalid parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.models import  AANBertGRU\n",
    "from model.criterion import MMD_loss\n",
    "from transformers import BertModel\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    " \n",
    "aan_version='AAN-A'\n",
    "MU = 0.1\n",
    "\n",
    "dataset = ['book.json','cd.json','elec.json','kitchen.json']\n",
    "source_file =dataset[0]\n",
    "target_file = dataset[1]\n",
    "\n",
    "source_iterator, target_iterator, valid_iterator, TEXT = get_iterator_feature(source_file, target_file, BATCH_SIZE=128)\n",
    "\n",
    "\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 2\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.25\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "model = AANBertGRU(bert, HIDDEN_DIM,OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT, aan_version)\n",
    "\n",
    "### freeze the Bert model.\n",
    "for name, param in model.named_parameters():                \n",
    "    if name.startswith('bert'):\n",
    "        param.requires_grad = False\n",
    "\n",
    "if aan_version == 'AAN':\n",
    "    optimizer_task = optim.Adam(model.parameters())\n",
    "else:\n",
    "    optimizer_task = optim.Adam([{'params':model.extractor.parameters()},{\"params\":model.rnn.parameters()},{'params':model.predictor.parameters()},{'params':model.bert.parameters()}])\n",
    "    optimizer_kernel = optim.Adam([{'params':model.mmd_linear.parameters()},{'params':model.cmmd_linear.parameters()}])\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "mmd_loss = MMD_loss(kernel_type='mmd', kernel_mul=2.0, kernel_num=5)\n",
    "cmmd_loss = MMD_loss(kernel_type='cmmd', kernel_mul=2.0, kernel_num=5,eplison=0.00001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training AAN (AAN-A) models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch: 01 | Epoch Time: 16m 50s |Best Epoch:0\n\tTrain Loss: -0.168|Valid Acc: 0.870\nEpoch: 02 | Epoch Time: 16m 55s |Best Epoch:1\n\tTrain Loss: -0.139|Valid Acc: 0.912\nEpoch: 03 | Epoch Time: 16m 50s |Best Epoch:2\n\tTrain Loss: -0.143|Valid Acc: 0.932\nEpoch: 04 | Epoch Time: 16m 55s |Best Epoch:3\n\tTrain Loss: -0.130|Valid Acc: 0.932\nEpoch: 05 | Epoch Time: 16m 49s |Best Epoch:3\n\tTrain Loss: -0.131|Valid Acc: 0.925\nEpoch: 06 | Epoch Time: 16m 50s |Best Epoch:3\n\tTrain Loss: -0.133|Valid Acc: 0.902\nEpoch: 07 | Epoch Time: 16m 48s |Best Epoch:3\n\tTrain Loss: -0.136|Valid Acc: 0.917\nEpoch: 08 | Epoch Time: 16m 50s |Best Epoch:3\n\tTrain Loss: -0.144|Valid Acc: 0.930\nEpoch: 09 | Epoch Time: 16m 47s |Best Epoch:3\n\tTrain Loss: -0.131|Valid Acc: 0.925\nEpoch: 10 | Epoch Time: 16m 48s |Best Epoch:3\n\tTrain Loss: -0.132|Valid Acc: 0.917\n"
    }
   ],
   "source": [
    "import time\n",
    "from model.tools import train_adverisal, train_normal, epoch_time, evaluate\n",
    "\n",
    "\n",
    "N_EPOCHS = 10\n",
    "best_loss = 100.0\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    if aan_version == 'AAN-A':\n",
    "        train_loss = train_adverisal(model,source_iterator,target_iterator,optimizer_task,optimizer_kernel,criterion,mmd_loss,cmmd_loss)\n",
    "    else:\n",
    "        train_loss = train_normal(model,source_iterator,target_iterator,optimizer_task,criterion,mmd_loss,cmmd_loss,MU)\n",
    "\n",
    "    eval_acc, eval_loss = evaluate(model, valid_iterator, criterion)\n",
    "    if eval_loss < best_loss:\n",
    "        best_loss = eval_loss\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(),'bert-aan-model.pt')\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s |Best Epoch:{best_epoch}',flush=True)\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}|Valid Acc: {eval_acc:.3f}',flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test AAN models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "from book.json to cd.json, acc is 0.929082\n"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('bert-aan-model.pt'))\n",
    "eval_acc, eval_loss = evaluate(model,target_iterator,criterion)\n",
    "print('from %s to %s, acc is %f'%(source_file,target_file, eval_acc),flush=True)"
   ]
  }
 ]
}