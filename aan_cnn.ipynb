{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextCNN-based AAN  models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### All the test is based on torch-1.2.0 and torchtext-0.6.0\n",
    "\n",
    "\n",
    "import torch\n",
    "from torchtext import data\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.nn import functional as F\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "[nltk_data] Error loading stopwords: <urlopen error [Errno 111]\n[nltk_data]     Connection refused>\n[nltk_data] Error loading wordnet: <urlopen error [Errno 111]\n[nltk_data]     Connection refused>\n"
    },
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### download stopword and wordnet using nltk\n",
    "\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare data loader based on torchtext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import Vectors\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "\n",
    "### pre-processing \n",
    "def tokenizer2(text):\n",
    "    regtokenizer = RegexpTokenizer(r'\\w+')\n",
    "    wnl = WordNetLemmatizer()\n",
    "    tokens = regtokenizer.tokenize(text)\n",
    "    \n",
    "    # remove stopwords\n",
    "    # stop = stopwords.words('english')\n",
    "    # tokens = [token for token in tokens if token not in stop]\n",
    "    \n",
    "    # tokens = [wnl.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    tokenized_text = []\n",
    "    auxiliary_verbs = ['am', 'is', 'are', 'was', 'were', \"'s\"]\n",
    "    for token in tokens:\n",
    "        if token == \"n't\":\n",
    "            tmp = 'not'\n",
    "        elif token == \"'ll\":\n",
    "            tmp = 'will'\n",
    "        elif token in auxiliary_verbs:\n",
    "            tmp = 'be'\n",
    "        else:\n",
    "            tmp = token\n",
    "        tokenized_text.append(tmp)\n",
    "    \n",
    "    return tokenized_text\n",
    "\n",
    "def get_iterator_feature(source_file, target_file, BATCH_SIZE=128,MAX_VOCAB_SIZE=20000):\n",
    "    TEXT = data.Field(sequential=True, tokenize=tokenizer2,lower=True,  fix_length=300,  batch_first=True)\n",
    "    LABEL = data.LabelField()\n",
    "\n",
    "    fields = {'review': ('text', TEXT), 'label': ('label', LABEL)}\n",
    "\n",
    "    train_data = data.TabularDataset.splits(\n",
    "                            path = 'datasets'+os.sep+\"amazon_text\",\n",
    "                            train = source_file,\n",
    "                            format = 'json',\n",
    "                            fields = fields\n",
    "    )\n",
    "    test_data = data.TabularDataset.splits(\n",
    "                            path = 'datasets'+os.sep+\"amazon_text\",\n",
    "                            train = target_file,\n",
    "                            format = 'json',\n",
    "                            fields = fields\n",
    "    )\n",
    "\n",
    "    train_data = train_data[0]\n",
    "\n",
    "    test_data = test_data[0]\n",
    "\n",
    "\n",
    "    test_data, valid_data = test_data.split(random_state = random.seed(SEED), split_ratio=0.95)\n",
    "\n",
    "    # MAX_VOCAB_SIZE = 20_000\n",
    "\n",
    "    TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "    LABEL.build_vocab(train_data)\n",
    "\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    source_iterator, target_iterator, valid_iterator = data.BucketIterator.splits(\n",
    "        (train_data, test_data, valid_data), \n",
    "        batch_size = BATCH_SIZE, \n",
    "        sort=False,\n",
    "        shuffle = True,\n",
    "        # repeat=True,\n",
    "        device = device)\n",
    "\n",
    "    return source_iterator, target_iterator, valid_iterator, TEXT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize AAN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.models import  AANTextCNN\n",
    "from model.criterion import MMD_loss\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "aan_version='AAN-A'\n",
    "\n",
    "dataset = ['book.json','cd.json','elec.json','kitchen.json']\n",
    "source_file =dataset[0]\n",
    "target_file = dataset[1]\n",
    "\n",
    "\n",
    "\n",
    "source_iterator, target_iterator, valid_iterator, TEXT = get_iterator_feature(source_file, target_file, BATCH_SIZE=256,MAX_VOCAB_SIZE=20000)\n",
    "\n",
    "\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_SIZE = 100\n",
    "LATENT_DIM = 100\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [8,9,10]\n",
    "OUTPUT_DIM = 2\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "MU = 0.1\n",
    "\n",
    "model = AANTextCNN(INPUT_DIM,EMBEDDING_SIZE,N_FILTERS,FILTER_SIZES, LATENT_DIM,OUTPUT_DIM, PAD_IDX,DROPOUT,aan_version)\n",
    "model.extractor.embedding.weight.data.copy_(TEXT.vocab.vectors)\n",
    "\n",
    "\n",
    "if aan_version == 'AAN':\n",
    "    optimizer_task = optim.Adam(model.parameters())\n",
    "else:\n",
    "    optimizer_task = optim.Adam([{'params':model.extractor.parameters()},{'params':model.predictor.parameters()}])\n",
    "    optimizer_kernel = optim.Adam([{'params':model.mmd_linear.parameters()},{'params':model.cmmd_linear.parameters()}])\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "mmd_loss = MMD_loss(kernel_type='mmd', kernel_mul=2.0, kernel_num=5)\n",
    "cmmd_loss = MMD_loss(kernel_type='cmmd', kernel_mul=2.0, kernel_num=5,eplison=0.00001)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training AAN (AAN-A) models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch: 01 | Epoch Time: 0m 44s |Best Epoch:0\n\tTrain Loss: -0.161|Valid Acc: 0.782\nEpoch: 02 | Epoch Time: 0m 44s |Best Epoch:1\n\tTrain Loss: -0.164|Valid Acc: 0.850\nEpoch: 03 | Epoch Time: 0m 45s |Best Epoch:2\n\tTrain Loss: -0.160|Valid Acc: 0.845\nEpoch: 04 | Epoch Time: 0m 45s |Best Epoch:3\n\tTrain Loss: -0.151|Valid Acc: 0.856\nEpoch: 05 | Epoch Time: 0m 45s |Best Epoch:4\n\tTrain Loss: -0.147|Valid Acc: 0.861\nEpoch: 06 | Epoch Time: 0m 45s |Best Epoch:5\n\tTrain Loss: -0.143|Valid Acc: 0.858\nEpoch: 07 | Epoch Time: 0m 45s |Best Epoch:6\n\tTrain Loss: -0.138|Valid Acc: 0.870\nEpoch: 08 | Epoch Time: 0m 44s |Best Epoch:7\n\tTrain Loss: -0.137|Valid Acc: 0.865\nEpoch: 09 | Epoch Time: 0m 45s |Best Epoch:7\n\tTrain Loss: -0.136|Valid Acc: 0.859\nEpoch: 10 | Epoch Time: 0m 44s |Best Epoch:9\n\tTrain Loss: -0.131|Valid Acc: 0.865\nEpoch: 11 | Epoch Time: 0m 44s |Best Epoch:10\n\tTrain Loss: -0.131|Valid Acc: 0.867\nEpoch: 12 | Epoch Time: 0m 44s |Best Epoch:10\n\tTrain Loss: -0.136|Valid Acc: 0.864\nEpoch: 13 | Epoch Time: 0m 44s |Best Epoch:10\n\tTrain Loss: -0.137|Valid Acc: 0.864\nEpoch: 14 | Epoch Time: 0m 44s |Best Epoch:10\n\tTrain Loss: -0.158|Valid Acc: 0.857\nEpoch: 15 | Epoch Time: 0m 44s |Best Epoch:10\n\tTrain Loss: -0.143|Valid Acc: 0.819\nEpoch: 16 | Epoch Time: 0m 45s |Best Epoch:15\n\tTrain Loss: -0.142|Valid Acc: 0.867\nEpoch: 17 | Epoch Time: 0m 45s |Best Epoch:15\n\tTrain Loss: -0.140|Valid Acc: 0.860\nEpoch: 18 | Epoch Time: 0m 44s |Best Epoch:15\n\tTrain Loss: -0.151|Valid Acc: 0.863\nEpoch: 19 | Epoch Time: 0m 44s |Best Epoch:15\n\tTrain Loss: -0.143|Valid Acc: 0.859\nEpoch: 20 | Epoch Time: 0m 44s |Best Epoch:15\n\tTrain Loss: -0.148|Valid Acc: 0.861\n"
    }
   ],
   "source": [
    "import time\n",
    "from model.tools import train_adverisal,  train_normal,epoch_time, evaluate\n",
    "\n",
    "N_EPOCHS = 20\n",
    "best_loss = 100.0\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    if aan_version == 'AAN-A':\n",
    "        train_loss = train_adverisal(model,source_iterator,target_iterator,optimizer_task,optimizer_kernel,criterion,mmd_loss,cmmd_loss)\n",
    "    else:\n",
    "        train_loss = train_normal(model,source_iterator,target_iterator,optimizer_task,criterion,mmd_loss,cmmd_loss,MU)\n",
    "\n",
    "    eval_acc, eval_loss = evaluate(model, valid_iterator, criterion)\n",
    "    if eval_loss < best_loss:\n",
    "        best_loss = eval_loss\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(),'aan-cnn-model.pt')\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s |Best Epoch:{best_epoch}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}|Valid Acc: {eval_acc:.3f}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test AAN models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "from book.json to cd.json, acc is 0.861947\n"
    }
   ],
   "source": [
    "from model.tools import evaluate\n",
    "\n",
    "### test the model.\n",
    "model.load_state_dict(torch.load('aan-cnn-model.pt'))\n",
    "eval_acc,eval_loss  = evaluate(model,target_iterator,criterion)\n",
    "print('from %s to %s, acc is %f'%(source_file,target_file, eval_acc))"
   ]
  }
 ]
}