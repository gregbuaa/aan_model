{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitpy37condadf10a4f515464ca38b9285a89fc4b8ae",
   "display_name": "Python 3.7.6 64-bit ('py37': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BertGRU-based ANN  models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### All the test is based on torch-1.2.0 and torchtext-0.6.0\n",
    "\n",
    "\n",
    "import torch\n",
    "from torchtext import data\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from torch.nn import functional as F\n",
    "from model.tools import categorical_accuracy,epoch_time\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare data loader based on torchtext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "init_token = tokenizer.cls_token\n",
    "eos_token = tokenizer.sep_token\n",
    "pad_token = tokenizer.pad_token\n",
    "unk_token = tokenizer.unk_token\n",
    "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
    "eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
    "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
    "\n",
    "def tokenize_and_cut(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence) \n",
    "    tokens = tokens[:max_input_length-2]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def get_iterator_feature(source_file, target_file, BATCH_SIZE=128):\n",
    "    TEXT = data.Field(batch_first = True,\n",
    "                  use_vocab = False,\n",
    "                  tokenize = tokenize_and_cut,\n",
    "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "                  init_token = init_token_idx,\n",
    "                  eos_token = eos_token_idx,\n",
    "                  pad_token = pad_token_idx,\n",
    "                  unk_token = unk_token_idx)\n",
    "\n",
    "    LABEL = data.LabelField(dtype = torch.long)\n",
    "\n",
    "    fields = {'review': ('text', TEXT), 'label': ('label', LABEL)}\n",
    "    # source_file = 'elec.json'\n",
    "    train_data = data.TabularDataset.splits(\n",
    "                        path = 'datasets'+os.sep+\"amazon_text\",\n",
    "                        train = source_file,\n",
    "                        format = 'json',\n",
    "                        fields = fields\n",
    "    )\n",
    "\n",
    "    test_data = data.TabularDataset.splits(\n",
    "                            path = 'datasets'+os.sep+\"amazon_text\",\n",
    "                            train = target_file,\n",
    "                            format = 'json',\n",
    "                            fields = fields\n",
    "    )\n",
    "\n",
    "    train_data = train_data[0]\n",
    "\n",
    "    test_data = test_data[0]\n",
    "\n",
    "\n",
    "    test_data, valid_data = test_data.split(random_state = random.seed(SEED), split_ratio=0.98)\n",
    "\n",
    "\n",
    "    LABEL.build_vocab(train_data)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    source_iterator, target_iterator,valid_iterator = data.BucketIterator.splits(\n",
    "        (train_data, test_data, valid_data), \n",
    "        batch_size = BATCH_SIZE, \n",
    "        sort=False,\n",
    "        shuffle = True,\n",
    "        device = device)\n",
    "\n",
    "    return source_iterator, target_iterator, valid_iterator, TEXT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize ANN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-1-9c44f670ff1c>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-9c44f670ff1c>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    source_file =dataset[i]\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "from model.models import  ANNBertGRU\n",
    "from model.criterion import MMD_loss\n",
    "from transformers import BertModel\n",
    "\n",
    "\n",
    "ann_version='ANN-A'\n",
    "\n",
    "dataset = ['book.json','cd.json','elec.json','kitchen.json']\n",
    "source_file =dataset[0]\n",
    "target_file = dataset[1]\n",
    "\n",
    "\n",
    "\n",
    "source_iterator, target_iterator, valid_iterator, TEXT = get_iterator_feature(source_file, target_file, BATCH_SIZE=256,MAX_VOCAB_SIZE=20000)\n",
    "\n",
    "\n",
    "source_iterator, target_iterator, valid_iterator, TEXT = get_iterator_feature(source_file, target_file, BATCH_SIZE=128)\n",
    "\n",
    "\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 2\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.25\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "MU = 0.1\n",
    "\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "model = ANNBertGRU(bert, HIDDEN_DIM,OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT, ann_version)\n",
    "\n",
    "### freeze the Bert model.\n",
    "for name, param in model.named_parameters():                \n",
    "    if name.startswith('bert'):\n",
    "        param.requires_grad = False\n",
    "\n",
    "if ann_version == 'ANN':\n",
    "    optimizer_task = optim.Adam(model.parameters())\n",
    "else:\n",
    "    optimizer_task = optim.Adam([{'params':model.extractor.parameters()},{\"params\":model.rnn.parameters()},{'params':model.predictor.parameters()},{'params':model.bert.parameters()}])\n",
    "    optimizer_kernel = optim.Adam([{'params':model.mmd_linear.parameters()},{'params':model.cmmd_linear.parameters()}])\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "mmd_loss = MMD_loss(kernel_type='mmd', kernel_mul=2.0, kernel_num=5)\n",
    "cmmd_loss = MMD_loss(kernel_type='cmmd', kernel_mul=2.0, kernel_num=5,eplison=0.00001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training ANN (ANN-A) models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 10\n",
    "best_loss = 100.0\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    # alpha = 0.7\n",
    "    start_time = time.time()\n",
    "    if ann_version == 'ANN-A':\n",
    "        train_loss = train_adverisal(model,source_iterator,target_iterator,optimizer_task,optimizer_kernel,criterion,mmd_loss,cmmd_loss)\n",
    "    else:\n",
    "        train_loss = train_normal(model,source_iterator,target_iterator,optimizer_task,criterion,mmd_loss,cmmd_loss,MU)\n",
    "\n",
    "    eval_acc, eval_loss = evaluate(model, valid_iterator, criterion)\n",
    "    if eval_loss < best_loss:\n",
    "        best_loss = eval_loss\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(),'bert-ann-model.pt')\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s |Best Epoch:{best_epoch}',flush=True)\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}|Valid Acc: {eval_acc:.3f}',flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test ANN models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('bert-ann-model.pt'))\n",
    "eval_acc, eval_loss = evaluate(model,target_iterator,criterion)\n",
    "print('from %s to %s, acc is %f'%(source_file,target_file, eval_acc),flush=True)"
   ]
  }
 ]
}