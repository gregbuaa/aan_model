{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitpy37condadf10a4f515464ca38b9285a89fc4b8ae",
   "display_name": "Python 3.7.6 64-bit ('py37': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextCNN-based ANN  models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### All the test is based on torch-1.2.0 and torchtext-0.6.0\n",
    "\n",
    "\n",
    "import torch\n",
    "from torchtext import data\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.nn import functional as F\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare data loader based on torchtext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import Vectors\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "### pre-processing \n",
    "spacy_en = spacy.load('en')\n",
    "def tokenizer2(text):\n",
    "    regtokenizer = RegexpTokenizer(r'\\w+')\n",
    "    wnl = WordNetLemmatizer()\n",
    "    tokens = regtokenizer.tokenize(text)\n",
    "    \n",
    "    # remove stopwords\n",
    "    stop = stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stop]\n",
    "    \n",
    "    tokens = [wnl.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    tokenized_text = []\n",
    "    auxiliary_verbs = ['am', 'is', 'are', 'was', 'were', \"'s\"]\n",
    "    for token in tokens:\n",
    "        if token == \"n't\":\n",
    "            tmp = 'not'\n",
    "        elif token == \"'ll\":\n",
    "            tmp = 'will'\n",
    "        elif token in auxiliary_verbs:\n",
    "            tmp = 'be'\n",
    "        else:\n",
    "            tmp = token\n",
    "        tokenized_text.append(tmp)\n",
    "    \n",
    "    return tokenized_text\n",
    "\n",
    "def get_iterator_feature(source_file, target_file, BATCH_SIZE=128,MAX_VOCAB_SIZE=20000):\n",
    "    TEXT = data.Field(sequential=True, tokenize=tokenizer2,lower=True,  fix_length=300,  batch_first=True)\n",
    "    LABEL = data.LabelField()\n",
    "\n",
    "    fields = {'review': ('text', TEXT), 'label': ('label', LABEL)}\n",
    "\n",
    "    train_data = data.TabularDataset.splits(\n",
    "                            path = 'datasets'+os.sep+\"amazon_text\",\n",
    "                            train = source_file,\n",
    "                            format = 'json',\n",
    "                            fields = fields\n",
    "    )\n",
    "    test_data = data.TabularDataset.splits(\n",
    "                            path = 'datasets'+os.sep+\"amazon_text\",\n",
    "                            train = target_file,\n",
    "                            format = 'json',\n",
    "                            fields = fields\n",
    "    )\n",
    "\n",
    "    train_data = train_data[0]\n",
    "\n",
    "    test_data = test_data[0]\n",
    "\n",
    "\n",
    "    test_data, valid_data = test_data.split(random_state = random.seed(SEED), split_ratio=0.95)\n",
    "\n",
    "    # MAX_VOCAB_SIZE = 20_000\n",
    "\n",
    "    TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "    LABEL.build_vocab(train_data)\n",
    "\n",
    "    TEXT.build_vocab(test_data, max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "    LABEL.build_vocab(test_data)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    source_iterator, target_iterator, valid_iterator = data.BucketIterator.splits(\n",
    "        (train_data, test_data, valid_data), \n",
    "        batch_size = BATCH_SIZE, \n",
    "        sort=False,\n",
    "        shuffle = True,\n",
    "        # repeat=True,\n",
    "        device = device)\n",
    "\n",
    "    return source_iterator, target_iterator, valid_iterator, TEXT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize ANN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-1-9c44f670ff1c>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-9c44f670ff1c>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    source_file =dataset[i]\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "from model.models import  ANNCNN\n",
    "from model.criterion import MMD_loss\n",
    "\n",
    "\n",
    "ann_version='ANN-A'\n",
    "\n",
    "dataset = ['book.json','cd.json','elec.json','kitchen.json']\n",
    "source_file =dataset[0]\n",
    "target_file = dataset[1]\n",
    "\n",
    "\n",
    "\n",
    "source_iterator, target_iterator, valid_iterator, TEXT = get_iterator_feature(source_file, target_file, BATCH_SIZE=256,MAX_VOCAB_SIZE=20000)\n",
    "\n",
    "\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_SIZE = 100\n",
    "LATENT_DIM = 100\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [8,9,10]\n",
    "OUTPUT_DIM = 2\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "MU = 0.1\n",
    "\n",
    "model = ANNadvCNN(INPUT_DIM,EMBEDDING_SIZE,N_FILTERS,FILTER_SIZES, LATENT_DIM,OUTPUT_DIM, PAD_IDX,DROPOUT,ann_version)\n",
    "model.extractor.embedding.weight.data.copy_(TEXT.vocab.vectors)\n",
    "\n",
    "\n",
    "if ann_version == 'ANN':\n",
    "    optimizer_task = optim.Adam(model.parameters())\n",
    "else:\n",
    "    optimizer_task = optim.Adam([{'params':model.extractor.parameters()},{'params':model.predictor.parameters()}])\n",
    "    optimizer_kernel = optim.Adam([{'params':model.mmd_linear.parameters()},{'params':model.cmmd_linear.parameters()}])\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "mmd_loss = MMD_loss(kernel_type='mmd', kernel_mul=2.0, kernel_num=5)\n",
    "cmmd_loss = MMD_loss(kernel_type='cmmd', kernel_mul=2.0, kernel_num=5,eplison=0.00001)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training ANN (ANN-A) models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.tools import train_adverisal,  train_normal,epoch_time\n",
    "\n",
    "N_EPOCHS = 20\n",
    "best_loss = 100.0\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    # alpha = 0.7\n",
    "    start_time = time.time()\n",
    "    if ann_version == 'ANN-A':\n",
    "        train_loss = train_adverisal(model,source_iterator,target_iterator,optimizer_task,optimizer_kernel,criterion,mmd_loss,cmmd_loss)\n",
    "    else:\n",
    "        train_loss = train_normal(model,source_iterator,target_iterator,optimizer_task,criterion,mmd_loss,cmmd_loss,MU)\n",
    "\n",
    "    eval_acc, eval_loss = evaluate(model, valid_iterator, criterion)\n",
    "    if eval_loss < best_loss:\n",
    "        best_loss = eval_loss\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(),'aan-cnn-model.pt')\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s |Best Epoch:{best_epoch}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}|Valid Acc: {eval_acc:.3f}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test ANN models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.tools import evaluate\n",
    "\n",
    "### test the model.\n",
    "model.load_state_dict(torch.load('aan-cnn-model.pt'))\n",
    "eval_acc,eval_loss  = evaluate(model,target_iterator,criterion)\n",
    "print('from %s to %s, acc is %f'%(source_file,target_file, eval_acc))"
   ]
  }
 ]
}